\documentclass{sig-alternate}

\begin{document}
%
% --- Author Metadata here ---
\conferenceinfo{SIGIR}{'2016 Pisa, Italy}

\title{Detecting significant events in news corpora}

\maketitle
\begin{abstract}
Forthcoming.
\end{abstract}

% A category with the (minimum) three required fields
\category{H.3.3}{Information Search and Retrieval}{}

\terms{}

\keywords{Event detection}

\section{Introduction}

In this paper, we present a method for discovering significant events in news corpora. We propose a novel unsupervised method for event detection
and introduce a test collection and evaluation methodology that can be used for future research.

\section{Use Cases}

Imagine a Wikipedia editor creating the `year' page for a particular year (e.g., https://en.wikipedia.org/wiki/1988). These pages contain lists of important events that occurred during that year. Given a corpus of news articles (or other timestamped collection), one use case is to show the editor a list of major events that happened during a given year  (or other time period) to facilitate the creation or validation of these pages.

A more general use case might be to summarize events in news corpora independent of Wikipedia. In this case, the user is looking for a list of major events that occurred in a given time span. 

While newswire feeds are obviously amenable to this type of analysis, other timestamped document collections or streams (e.g., Twitter, Blogs) might also be useful.

\section{What is an event?}

The concepts of `event detection' or `event extraction' are common in the IR, NLP and ML literature. A variety of different definitions and operationalizations of `events' are commonly used.  In this paper, we are concerned with the retrospective identification of large-scale events in historical news corpora. A number of studies have used the concepts of ``significant,'' ``seminal,'' or ``newsworthy'' events to describe such large-scale events.


\section{Related Work}

TBD

\section{Our model}

Retrospective event detection methods are generally of two types: document-centric or feature-centric.  In document-centric event detection, documents are first clustered based on their content, then interesting temporal signals are identified from the clusters.  In feature-centric event detection, the temporal distribution of individual terms are used as signals to identify events. Terms are then clustered into event models.  

This study proposes a novel approach to feature-centric retrospective event detection. The model works as follows:
\begin{enumerate}
\item We construct a temporal index from the document collection. The index consists of document frequencies for each term in the vocabulary at a particular interval (e.g., hour, day, week).
\item For each term in the vocabulary, we construct a time series based on the distribution of the term frequencies over time. We then calculate the first-order auto-correlation ($\rho$) of the time series. Terms with high temporal dependency will have very high (1) or very low (-1) $\rho$.  Terms with $\rho$ values closer to 0 have low temporal dependency. 
\item We assume that most terms in the vocabulary are not indicative of events. We assume that $\rho \sim N(\mu, \sigma)$ and that event-indicating terms are generated by a different process than non-event indicating terms.  We test the hypothesis that term $t$ is drawn from the non-event indicating distribution.  If $p-value < \alpha$, reject the null hypothesis.
\item For all terms drawn from the event-indicating distribution, calculate the temporal mutual information $I_t(x;y)$ between terms.
\item Construct a matrix of summary temporal mutual information between terms (e.g, maximum, average, minimum, variance). Use non-negative matrix factorization (NMF) to find the top $k$ factors
\item Use the NMF factor weights to create the event model.
\end{enumerate}

The resulting event model can be evaluated qualitatively (ala topic models) or used as a synthetic query to retrieve information about the event in the document collection or in an external collection, such as Wikipedia.


\section{Evaluation}
There are three primary concerns in the evaluation of this approach:
\begin{enumerate}
\item Did the model find all of the significant events in the corpus? To assess this, we need a ground truth containing a comprehensive list of events in the corpus. 
\item How many of the identified `events' are truly events? In this case, we need to evaluate whether the events identified by the system correspond to events in the corpus.  
\item How accurate are the identified date constraints? To address this question, we need the start and end dates associated with each identified event.
\end{enumerate}

For this study, we develop an evaluation methodology based on Wikipedia `year' pages supplemented with the manual assessment of candidate events returned by the systems.

\subsection{Document collection}
News articles from the years 2004 and 2005 in the New York Times Annotated Corpus are used for evaluation with stop words removed. 

\subsection{Wikipedia year pages}
Using the Wikipedia `year' pages, we construct an initial ground-truth of events. For each listed event, we extract the description and associated dates. Each identified event is then manually reviewed by two assessors to 1) assign an importance level to the event (described below) and 2) to identify the Wikipedia page that best represents the event.  

A central concept in the evaluation of the system is event \emph{importance}. One goal of the ground truth construction is to identify major events from the perspective of US news media. We define the following three categories:
\begin{enumerate}
\item Major: News articles about this event are likely to be found on the front-page. Interest in the event spans more than one week.
\item Moderate: News articles about this event may be found on the front-page. Interest in the event spans less than one week.
\item Minor: News articles about this event are unlikely to be found on the front-page. Interest in the event may only span a day or two.
\end{enumerate}

Each assessor is asked to review the list of events, assign an importance level to the event, and identify the best Wikipedia page that describes the event, if present.

One obvious limitation of this approach is the assumption that Wikipedia contains descriptions of all events or that the year pages are comprehensive.  We address this limitation through the qualitative analysis of events returned by the event detection systems, described next.

\subsection{Manual assessment}

For evaluation, each system returns the top $N$ candidate events. For each event, the system returns an estimated event date and the top $K$ Wikipedia pages that best represent each event.  Two assessors are tasked with manually determining whether the candidate events represent \emph{true} events and to collect additional details about the events for final evaluation.

For each event returned by the systems, two assessors are be tasked with reviewing the event models, results, and dates to assess the following:
\begin{enumerate}
\item Is this a ``real'' event (e.g., do you think the model is describing an event that really happened)?
\item If yes, what are the estimated constraints of the event (start/end dates)
\item Is the event represented in Wikipedia?
\item If yes, what is the best URL for the event?
\item If yes, how relevant are each of the results by the system (0=not relevant, 1=relevant, 2=the page is dedicated to the event)
\item What is the event importance (major/moderate/minor)?
\end{enumerate}

\subsection{Metrics}

For each system, we can calculate recall and precision based on the combined lists of events identified by the Wikipedia editors and events identified by the systems. How many of the true `major' events did the system identify? How many of the identified events are true `major events'?  Using the top $K$ Wikipedia pages, we can also calculate IR metrics such as NDCG (how highly ranked are the best URLs for a particular event with respect to the event model)?

\section{Conclusions}
TBD

%ACKNOWLEDGMENTS are optional
\section{Acknowledgments}
This section is optional

%
% The following two commands are all you need in the
% initial runs of your .tex file to
% produce the bibliography for the citations in your paper.


\bibliographystyle{abbrv}
\bibliography{temporalir}  

\appendix

\end{document}
